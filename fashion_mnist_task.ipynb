{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b97009-91e4-4c88-bcdb-4ef9174c7778",
   "metadata": {},
   "source": [
    "# MNIST Fashion Task for MLP implementation with PyTorch\n",
    "### Initial preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f82a13be-1e5f-4489-9714-24eaecf11d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# PyTorch for MLP\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "# Dataset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9462484-2a53-4c4e-ac46-aba3d9ad00a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seeds\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3305f1a-a2aa-43f9-a474-e43ebbf353f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a18006-06ae-441d-bbf5-62c215743a9f",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5235d229-e5e9-411a-b51c-b4a341451f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FashionMNIST training dataset\n",
    "fashion = FashionMNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b55b66e-3c3e-49aa-880a-7607a3e5868d",
   "metadata": {},
   "source": [
    "### Prepare the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5d3f93e-ab6c-40ae-85f4-16802e96c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split input features and labels\n",
    "# Initialise lists\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Iterate through the data and split it\n",
    "for image, label in fashion:\n",
    "    X.append(image.flatten().numpy())\n",
    "    y.append(label)\n",
    "\n",
    "# Convert to numpy array\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5826da84-cdd7-4304-b4c7-4b8a0660f113",
   "metadata": {},
   "source": [
    "### Prepare the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "179fd1f3-dfdc-4683-a180-c21a05614dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure network architecture\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()  # Secures inheritance from the MLP class\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Fully Connected (dense) layers\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b486be4e-2c1c-4086-ac0a-69a30976cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set architecture constants\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eb7ab9-9b12-4c8d-a51f-cb38d6998717",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "238171c9-4869-4842-8dff-fe6f363c89aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.3046\n",
      "Epoch [2/10], Loss: 2.3045\n",
      "Epoch [3/10], Loss: 2.3043\n",
      "Epoch [4/10], Loss: 2.3042\n",
      "Epoch [5/10], Loss: 2.3040\n",
      "Epoch [6/10], Loss: 2.3039\n",
      "Epoch [7/10], Loss: 2.3037\n",
      "Epoch [8/10], Loss: 2.3035\n",
      "Epoch [9/10], Loss: 2.3034\n",
      "Epoch [10/10], Loss: 2.3032\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# Build the network\n",
    "mlp = MLP(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Select the optimiser\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=0.01)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = mlp(X_tensor)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b1eae-01a1-4565-9435-748c551e421c",
   "metadata": {},
   "source": [
    "### Evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c62ad1e2-62c6-4ece-b969-d50bda0e3c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [5 3 3 ... 5 7 7]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    predictions = torch.argmax(mlp(X_tensor), dim=1)\n",
    "print(f\"Predictions: {predictions.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "580d4040-b325-4879-870c-2a958bc831ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "fashion_test = FashionMNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1def76a8-d453-407d-b535-612c46e073b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the test data for evaluation\n",
    "# Initialise lists\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "# Iterate through the data and split it\n",
    "for image, label in fashion_test:\n",
    "    X_test.append(image.flatten().numpy())\n",
    "    y_test.append(label)\n",
    "\n",
    "# Convert to numpy array\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df7f14fb-e1ce-425a-9c90-d3243f83e943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.09\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    # Run the model without gradients (ie with the weights arrived at at the\n",
    "    # end of training)\n",
    "    outputs_test = mlp(X_test_tensor)\n",
    "\n",
    "    # Assign classes to the output\n",
    "    _, predicted = torch.max(outputs_test, 1)  # Predicted will be a tensor\n",
    "\n",
    "    # Find accuracy using sklearn accuracy_score, which requires numpy array\n",
    "    accuracy = accuracy_score(y_test, predicted.numpy())\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391f945-2d4a-41f3-98a7-40e008ed7086",
   "metadata": {},
   "source": [
    "The initial implementation with: lr = 0.01, hidden_size = 64, epochs = 10 achieves test accuracy of 0.09 but the target is a test accuracy >= 0.80.\n",
    "\n",
    "Functionalise the above code for convenient testing and optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "920cbc33-aae6-4293-8ef7-52a43eadb613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_train_net(\n",
    "    hidden_size, learning_rate, epochs,\n",
    "    X_tensor, y_tensor, X_test_tensor, y_test\n",
    "):\n",
    "    '''\n",
    "    Function to train and evaluate a neural network with one fully-connected\n",
    "    hidden layer. It builds and trains the network, evaluating the model every\n",
    "    100 epochs. The function returns the selected parameters and the accuracy\n",
    "    of the final model.\n",
    "    Parameters:\n",
    "    hidden_size = number of neurons in the hidden layer\n",
    "    learning_rate = learning rate (0, 1)\n",
    "    epochs = number of training epochs, integer\n",
    "        X_tensor = training data as PyTorch tensor\n",
    "        X_test_tensor = test data as PyTorch tensor\n",
    "    Output:\n",
    "        result = list of hidden_size, learning_rate, epochs, accuracy\n",
    "    '''\n",
    "    # Build the network\n",
    "    mlp = MLP(input_size, hidden_size, output_size)\n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Select the optimiser\n",
    "    optimizer = optim.SGD(mlp.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the network\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        outputs = mlp(X_tensor)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate the network every 100 iterations\n",
    "        if epoch % 100 == 0:\n",
    "            # model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Run the model without gradients (ie with the weights arrived\n",
    "                # at at the end of training)\n",
    "                outputs_test = mlp(X_test_tensor)\n",
    "\n",
    "                # Assign classes to the output\n",
    "                _, predicted = torch.max(outputs_test, 1)\n",
    "\n",
    "            # Find accuracy using sklearn accuracy_score\n",
    "            accuracy = accuracy_score(y_test, predicted.numpy())\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{epochs}], Loss: {loss:.4f}, \"\n",
    "                f\"Accuracy: {accuracy:.2f}\"\n",
    "            )\n",
    "\n",
    "    result = [hidden_size, learning_rate, epochs, accuracy]\n",
    "    return result, mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38309eb0-cf04-40fe-92d9-98957dc61347",
   "metadata": {},
   "source": [
    "Baseline learning rate is slow and the number of epochs is low. Increasing both should improve the accuracy. Set learning rate 0.1 and extend the epochs to 2,001. By reporting the loss and accuracy every 100 epochs, I will be able to choose a suitable value for epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ccdb98e-d4a5-436b-85df-58a737deefcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2001], Loss: 2.3023, Accuracy: 0.14\n",
      "Epoch [101/2001], Loss: 1.8682, Accuracy: 0.67\n",
      "Epoch [201/2001], Loss: 1.7509, Accuracy: 0.74\n",
      "Epoch [301/2001], Loss: 1.7231, Accuracy: 0.76\n",
      "Epoch [401/2001], Loss: 1.7078, Accuracy: 0.77\n",
      "Epoch [501/2001], Loss: 1.6975, Accuracy: 0.78\n",
      "Epoch [601/2001], Loss: 1.6901, Accuracy: 0.78\n",
      "Epoch [701/2001], Loss: 1.6844, Accuracy: 0.78\n",
      "Epoch [801/2001], Loss: 1.6800, Accuracy: 0.79\n",
      "Epoch [901/2001], Loss: 1.6763, Accuracy: 0.79\n",
      "Epoch [1001/2001], Loss: 1.6731, Accuracy: 0.79\n",
      "Epoch [1101/2001], Loss: 1.6705, Accuracy: 0.79\n",
      "Epoch [1201/2001], Loss: 1.6681, Accuracy: 0.80\n",
      "Epoch [1301/2001], Loss: 1.6661, Accuracy: 0.80\n",
      "Epoch [1401/2001], Loss: 1.6643, Accuracy: 0.80\n",
      "Epoch [1501/2001], Loss: 1.6626, Accuracy: 0.80\n",
      "Epoch [1601/2001], Loss: 1.6612, Accuracy: 0.80\n",
      "Epoch [1701/2001], Loss: 1.6598, Accuracy: 0.80\n",
      "Epoch [1801/2001], Loss: 1.6586, Accuracy: 0.80\n",
      "Epoch [1901/2001], Loss: 1.6574, Accuracy: 0.80\n",
      "Epoch [2001/2001], Loss: 1.6564, Accuracy: 0.80\n",
      "[64, 0.1, 2001, 0.8023]\n"
     ]
    }
   ],
   "source": [
    "result, _ = eval_train_net(\n",
    "    hidden_size, 0.1, 2001, X_tensor, y_tensor, X_test_tensor, y_test_tensor\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5304ffbc-150c-4ba2-8639-772ad4cde1be",
   "metadata": {},
   "source": [
    "These results indicate 1200 epochs is sufficient to achieve accuracy >= 0.80."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c19e197-783d-444e-84e8-682021698ae5",
   "metadata": {},
   "source": [
    "### Select and tune a hyperparameter\n",
    "Hyperparameters control how a machine learning algorithm learns. They are user-defined constants in the training process, such as hidden_size, learning_rate and epochs. The baseline neural network was not particularly accurate, and by increasing learning_rate and epochs, I've improved the accuracy. I will fix the learning rate and epochs, although if a further significant improvement in performance occurs, it will be necessary to revisit both.\n",
    "\n",
    "An important hyperparameter that is _hidden_ in this implementation is the batch size. The batch size is the number of subsamples used to update the gradients. The best measure of the gradients is to use all the samples before updating. This is computationally expensive and can lead to training scenarios where the model gets \"stuck\" at a local minima. But this approach takes the most direct route to convergence on the solution. Methods that use all samples in this way have a batch size equal to the number of samples, and are called (full) Batch Gradient Descent (BGD). \n",
    "\n",
    "The extreme opposite approach to BGD is Stochastic Gradient Descent (SGD), which calculates the gradient for a single sample before updating ie batch size of 1. This introduces noise into the gradient, which can be helpful in enabling the model to escape local minima during training, but the approach to convergence is indirect. SGD is computationally cheap in the backward pass, so for some models this can be tolerated.\n",
    "\n",
    "Mini-Batch Gradient Descent (MBGD) lies between BGD and SGD, combining benefits of both. In this case the batch size becomes a tunable parameter capable of balancing computational cost and training stability. For classification problems batches are typically powers of 2 eg 32, 64, 128, 256 and 512. The batches are chosen without replacement. A training epoch completes once all the samples have been used. Since the largest power of 2 less than 60,000 is 2^15 =  32,768 and this is a lot less than 60,000 I will use multiples of 10 for my batches to avoid losing almost half of my training dataset. \n",
    "\n",
    "For this part of the task, I will tune batch size.\n",
    "\n",
    "Tutorial reference: https://codesignal.com/learn/courses/pytorch-techniques-for-model-optimization/lessons/model-training-with-mini-batches-in-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46daf781-2db2-4f84-9485-168cfffa3bc8",
   "metadata": {},
   "source": [
    "PyTorch DataLoader creates mini-batches from the dataset. Pytorch TensorDataset(X, y) combines X and y into one tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "643c4c29-23d2-4599-9b39-fc5fb46f226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8319eaee-5a1c-4c82-9bab-a4b82059eae4",
   "metadata": {},
   "source": [
    "Rework my eval_train_net function to fix hidden_size at 64, learning_rate at 0.1, and use DataLoader for the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce88506f-5c35-418a-afa6-74f48f761a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_batch(\n",
    "    X_tensor, y_tensor, X_test_tensor, y_test_tensor,\n",
    "    batch_size, epochs=10, hidden_size=64, learning_rate=0.1\n",
    "):\n",
    "    '''\n",
    "    Function to train and evaluate a neural network with one fully-connected\n",
    "    hidden layer. It builds and trains the network, evaluating the model every\n",
    "    100 epochs. The function returns the selected parameters and the accuracy\n",
    "    of the final model.\n",
    "    Parameters:\n",
    "    hidden_size = number of neurons in the hidden layer\n",
    "    learning_rate = learning rate (0, 1)\n",
    "    epochs = number of training epochs, integer\n",
    "        X_tensor = training data as PyTorch tensor\n",
    "        X_test_tensor = test data as PyTorch tensor\n",
    "    Output:\n",
    "        result = list of hidden_size, learning_rate, epochs, accuracy\n",
    "    '''\n",
    "    # Combine dataset into one tensor\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "    # Build the network\n",
    "    mlp = MLP(input_size, hidden_size, output_size)\n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Select the optimiser\n",
    "    optimizer = optim.SGD(mlp.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the network in mini-batches\n",
    "    for epoch in range(epochs):\n",
    "        # Draw random mini-batch\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Train the network on each mini-batch\n",
    "        for batch_X, batch_y in data_loader:\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = mlp(batch_X)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate the network every 2 iterations\n",
    "        if epoch % 2 == 0:\n",
    "            with torch.no_grad():\n",
    "                # Run the model without gradients (ie with the weights arrived\n",
    "                # at at the end of training)\n",
    "                outputs_test = mlp(X_test_tensor)\n",
    "\n",
    "                # Assign classes to the output\n",
    "                _, predicted = torch.max(outputs_test, 1)\n",
    "\n",
    "            # Find accuracy using sklearn accuracy_score\n",
    "            accuracy = accuracy_score(y_test, predicted.numpy())\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{epochs}], Loss: {loss:.4f}, \"\n",
    "                f\"Accuracy: {accuracy:.2f}\"\n",
    "            )\n",
    "\n",
    "    result = [batch_size, accuracy]\n",
    "    return result, mlp, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c2cbae-d2ef-4d7f-ba8f-6228bd721f1c",
   "metadata": {},
   "source": [
    "Test the eval_batch with batch_size = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9e81971-cc20-411f-8325-bf562a3469d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.6277, Accuracy: 0.81\n",
      "Epoch [3/10], Loss: 1.4615, Accuracy: 0.84\n",
      "Epoch [5/10], Loss: 1.5591, Accuracy: 0.84\n",
      "Epoch [7/10], Loss: 1.5436, Accuracy: 0.84\n",
      "Epoch [9/10], Loss: 1.5615, Accuracy: 0.85\n",
      "[10, 0.8527]\n",
      "MLP(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "0.8527\n"
     ]
    }
   ],
   "source": [
    "result, mlp, accuracy = eval_batch(\n",
    "    X_tensor, y_tensor, X_test_tensor, y_test_tensor, batch_size\n",
    ")\n",
    "print(result)\n",
    "print(mlp)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53daf685-3607-4119-95a4-ad87e3acb9a0",
   "metadata": {},
   "source": [
    "This shows a significant performance improvement both in run time (not recorded but I estimate 10-20 seconds rather than 1-2 minutes) and accuracy improves to 0.85.\n",
    "\n",
    "This is encouraging but it isn't hyperparameter tuning, since a batch size of 10 was simply chosen as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09c13c9b-970e-4583-b600-0b2f1cc1a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of batch_sizes to try\n",
    "batch_ls = [2, 4, 6, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "353b3386-75e1-44db-8d0a-191e38a4b34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.9611, Accuracy: 0.59\n",
      "Epoch [3/10], Loss: 1.4612, Accuracy: 0.67\n",
      "Epoch [5/10], Loss: 1.4612, Accuracy: 0.65\n",
      "Epoch [7/10], Loss: 1.4612, Accuracy: 0.67\n",
      "Epoch [9/10], Loss: 1.9612, Accuracy: 0.61\n",
      "Epoch [1/10], Loss: 1.7112, Accuracy: 0.75\n",
      "Epoch [3/10], Loss: 1.7112, Accuracy: 0.76\n",
      "Epoch [5/10], Loss: 1.4612, Accuracy: 0.79\n",
      "Epoch [7/10], Loss: 1.4612, Accuracy: 0.78\n",
      "Epoch [9/10], Loss: 1.4612, Accuracy: 0.79\n",
      "Epoch [1/10], Loss: 1.6278, Accuracy: 0.80\n",
      "Epoch [3/10], Loss: 1.6278, Accuracy: 0.82\n",
      "Epoch [5/10], Loss: 1.4613, Accuracy: 0.83\n",
      "Epoch [7/10], Loss: 1.6278, Accuracy: 0.83\n",
      "Epoch [9/10], Loss: 1.6278, Accuracy: 0.84\n",
      "Epoch [1/10], Loss: 1.5648, Accuracy: 0.78\n",
      "Epoch [3/10], Loss: 1.5614, Accuracy: 0.80\n",
      "Epoch [5/10], Loss: 1.7571, Accuracy: 0.81\n",
      "Epoch [7/10], Loss: 1.5612, Accuracy: 0.82\n",
      "Epoch [9/10], Loss: 1.6594, Accuracy: 0.80\n",
      "Epoch [1/10], Loss: 1.5884, Accuracy: 0.80\n",
      "Epoch [3/10], Loss: 1.7712, Accuracy: 0.81\n",
      "Epoch [5/10], Loss: 1.5940, Accuracy: 0.80\n",
      "Epoch [7/10], Loss: 1.5287, Accuracy: 0.82\n",
      "Epoch [9/10], Loss: 1.6632, Accuracy: 0.82\n",
      "Epoch [1/10], Loss: 1.7261, Accuracy: 0.79\n",
      "Epoch [3/10], Loss: 1.6612, Accuracy: 0.81\n",
      "Epoch [5/10], Loss: 1.7119, Accuracy: 0.80\n",
      "Epoch [7/10], Loss: 1.6440, Accuracy: 0.81\n",
      "Epoch [9/10], Loss: 1.6222, Accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Initialise result list of result tuples\n",
    "batch_results = []\n",
    "\n",
    "# Loop through batch_ls sizes\n",
    "for batch in batch_ls:\n",
    "    result, _, _ = eval_batch(\n",
    "        X_tensor, y_tensor, X_test_tensor, y_test_tensor, batch\n",
    "    )\n",
    "    batch_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29693abb-7b10-44a7-9baa-7a2ae305e6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Batch_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Accuracy_score",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "08e9ad05-4595-4afd-ae4c-6687866e1114",
       "rows": [
        [
         "0",
         "2",
         "0.6088"
        ],
        [
         "1",
         "4",
         "0.789"
        ],
        [
         "2",
         "6",
         "0.8362"
        ],
        [
         "3",
         "10",
         "0.7961"
        ],
        [
         "4",
         "15",
         "0.8182"
        ],
        [
         "5",
         "20",
         "0.8106"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch_size</th>\n",
       "      <th>Accuracy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.6088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.7890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0.8362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.7961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>0.8182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.8106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Batch_size  Accuracy_score\n",
       "0           2          0.6088\n",
       "1           4          0.7890\n",
       "2           6          0.8362\n",
       "3          10          0.7961\n",
       "4          15          0.8182\n",
       "5          20          0.8106"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the results using a pandas DataFrame\n",
    "df = pd.DataFrame(batch_results, columns=['Batch_size', 'Accuracy_score'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee617c34-5950-49a8-8ad2-b3796789aa87",
   "metadata": {},
   "source": [
    "The results show I was lucky with the batch size of 10 as a test run, since it does seem to be the best option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d41ee028-d8df-43e4-abb1-763dbcf48bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.8117, Accuracy: 0.78\n",
      "Epoch [3/20], Loss: 1.6694, Accuracy: 0.80\n",
      "Epoch [5/20], Loss: 1.5504, Accuracy: 0.80\n",
      "Epoch [7/20], Loss: 1.6221, Accuracy: 0.81\n",
      "Epoch [9/20], Loss: 1.6510, Accuracy: 0.81\n",
      "Epoch [11/20], Loss: 1.6749, Accuracy: 0.81\n",
      "Epoch [13/20], Loss: 1.6295, Accuracy: 0.81\n",
      "Epoch [15/20], Loss: 1.7970, Accuracy: 0.81\n",
      "Epoch [17/20], Loss: 1.6630, Accuracy: 0.82\n",
      "Epoch [19/20], Loss: 1.5642, Accuracy: 0.82\n",
      "[10, 0.8179]\n"
     ]
    }
   ],
   "source": [
    "# Revisiting learning_rate=0.01.\n",
    "result, mlp_0_01, accuracy = eval_batch(\n",
    "    X_tensor, y_tensor, X_test_tensor, y_test_tensor,\n",
    "    batch_size=10, epochs=20, hidden_size=64, learning_rate=0.01\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df13786-eafa-4457-80f9-cf0a21abc3d2",
   "metadata": {},
   "source": [
    "The learning rate of 0.01 has a similar performance to using 0.1 at 10 epochs. However, with more than 10 epochs, the accuracy continues to improve. This suggests 0.01 is a better choice, as it is more likely to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a4c8dc-5f4d-4196-9dd8-0f9f39c21378",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32902406-a0a1-4596-9843-4181eeaf2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    '''\n",
    "    Function provided in the task to evaluate the model. The code provided in\n",
    "    the task to print the confusion matrix:\n",
    "    pred, labels = evaluate_model(model, test_loader)\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(conf_matrix)\n",
    "    '''\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f35e1e62-8c59-4b60-9e76-88d452f63422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test_data\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Set batch size\n",
    "test_batch = 10\n",
    "\n",
    "# Prepare test_loader\n",
    "test_loader = DataLoader(test_data, batch_size=test_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "144c0bfc-9e32-4dd9-a798-23724c2e5bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[905   0  24  45   7   1   0   1  17   0]\n",
      " [  4 961   2  25   6   0   0   0   2   0]\n",
      " [ 23   4 794  17 154   1   0   0   7   0]\n",
      " [ 40  12  18 890  35   1   0   0   4   0]\n",
      " [  3   1  92  34 865   0   0   0   5   0]\n",
      " [  1   0   0   1   0 891   0  64   4  39]\n",
      " [315   1 230  51 379   0   0   0  24   0]\n",
      " [  0   0   0   0   0  15   0 932   1  52]\n",
      " [  5   1  10   7   4   4   0   8 960   1]\n",
      " [  0   0   0   0   0   7   0  32   1 960]]\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix\n",
    "preds, labels = evaluate_model(mlp_0_01, test_loader)\n",
    "conf_matrix = confusion_matrix(labels, preds)\n",
    "print(\"Confusion Matrix: \")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf47ede5-696a-42b4-bb62-a69347d6e69a",
   "metadata": {},
   "source": [
    "In a confusion matrix, the rows represent observed data (aka ground truth) and the columns represent predicted data. The NW-SE diagonal quantifies how good the model is at correctly predicting each class. Thus the confusion matrix for a perfect set of predictions would be a scalar multiple of the identity matrix ie all off-diagonal elements would be zero.\n",
    "\n",
    "Before interpreting the confusion matrix, let's check the distribution of unique values in preds (predictions) and labels (ground truth). Ideally the unique values will be uniformly distributed, if they are not, the imbalance can influence the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0b7aeb6-c43a-40a7-ab66-43f964cc8567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: \n",
      "dict_keys([8, 5, 9, 4, 0, 1, 3, 2, 7])\n",
      "dict_values([1025, 920, 1052, 1450, 1296, 980, 1070, 1170, 1037])\n",
      "Labels: \n",
      "dict_keys([8, 5, 9, 4, 0, 6, 1, 3, 2, 7])\n",
      "dict_values([1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000])\n"
     ]
    }
   ],
   "source": [
    "# Find the unique counts in preds and labels\n",
    "unique_pred = Counter(preds).keys()\n",
    "unique_pred_count = Counter(preds).values()\n",
    "\n",
    "unique_label = Counter(labels).keys()\n",
    "unique_label_count = Counter(labels).values()\n",
    "\n",
    "print(\"Predictions: \")\n",
    "print(unique_pred)\n",
    "print(unique_pred_count)\n",
    "\n",
    "print(\"Labels: \")\n",
    "print(unique_label)\n",
    "print(unique_label_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a5a798-d5f3-41e3-8610-d9991efe2302",
   "metadata": {},
   "source": [
    "The predicted and ground truth data unique values are evenly distributed so we can consider the confusion matrix. The rows with the most non-zero elements off the diagonal have indices 4 and 6. The model has the greatest difficulty correctly identifying those elements. In particular, this model never correctly identifies row 6.\n",
    "\n",
    "We can also see this from the classification report, where precision and f1-scores for rows 4 and 6 are significantly less than for the other rows. Recall is more variable across all the classes. The overall accuracy is 0.82."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8998f864-78aa-4336-9e5b-d634a067187c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.698     0.905     0.788      1000\n",
      "           1      0.981     0.961     0.971      1000\n",
      "           2      0.679     0.794     0.732      1000\n",
      "           3      0.832     0.890     0.860      1000\n",
      "           4      0.597     0.865     0.706      1000\n",
      "           5      0.968     0.891     0.928      1000\n",
      "           6      0.000     0.000     0.000      1000\n",
      "           7      0.899     0.932     0.915      1000\n",
      "           8      0.937     0.960     0.948      1000\n",
      "           9      0.913     0.960     0.936      1000\n",
      "\n",
      "    accuracy                          0.816     10000\n",
      "   macro avg      0.750     0.816     0.778     10000\n",
      "weighted avg      0.750     0.816     0.778     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "print(classification_report(labels, preds, digits=3, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732b331-ee86-4f4a-b665-0dc5d2c85f5e",
   "metadata": {},
   "source": [
    "Let's try the mlp model (ie learning rate 0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b54ff843-7535-4553-a502-766b4aca9806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[842   1   9  32   4   0  90   1  21   0]\n",
      " [  4 960   0  25   5   0   5   0   1   0]\n",
      " [ 15   2 698  18 134   0 129   0   4   0]\n",
      " [ 39  11   8 866  38   0  30   0   8   0]\n",
      " [  1   1  64  33 793   0 101   0   7   0]\n",
      " [  0   0   0   1   0 894   1  64   6  34]\n",
      " [161   1  75  38  77   0 626   1  21   0]\n",
      " [  0   0   0   0   0  16   0 957   0  27]\n",
      " [  0   1   1   7   4   2  14   6 965   0]\n",
      " [  0   0   0   0   0   6   0  58   1 935]]\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix\n",
    "preds, labels = evaluate_model(mlp, test_loader)\n",
    "conf_matrix = confusion_matrix(labels, preds)\n",
    "print(\"Confusion Matrix: \")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd78d993-af01-4431-bb80-4f3e044f14a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.793     0.842     0.817      1000\n",
      "           1      0.983     0.960     0.971      1000\n",
      "           2      0.816     0.698     0.753      1000\n",
      "           3      0.849     0.866     0.857      1000\n",
      "           4      0.752     0.793     0.772      1000\n",
      "           5      0.974     0.894     0.932      1000\n",
      "           6      0.629     0.626     0.627      1000\n",
      "           7      0.880     0.957     0.917      1000\n",
      "           8      0.933     0.965     0.949      1000\n",
      "           9      0.939     0.935     0.937      1000\n",
      "\n",
      "    accuracy                          0.854     10000\n",
      "   macro avg      0.855     0.854     0.853     10000\n",
      "weighted avg      0.855     0.854     0.853     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "print(classification_report(labels, preds, digits=3, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab671ba-bfea-47cb-9023-9225cdc654b9",
   "metadata": {},
   "source": [
    "This is a much better result as the model now predicts all possible outcomes. The classes 4 and 6 are still the most challenging for the model: these classes are coat and shirt, which are clearly more similar in shape than, say ankle boot and coat."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
